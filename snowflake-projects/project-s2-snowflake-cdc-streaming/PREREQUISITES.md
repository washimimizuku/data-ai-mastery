# Prerequisites: Real-Time CDC with Snowpipe Project

## Required Bootcamps

### 30 Days of Snowflake for Data and AI
**Mandatory Lessons:**
- Day 1: Snowflake Architecture & Account Setup
- Day 2: Virtual Warehouses & Compute Management
- Day 3: Databases, Schemas & Tables
- Day 4: Data Loading with Snowpipe
- Day 11: Streams & Change Data Capture
- Day 12: Tasks & Scheduling
- Day 13: Snowpipe Streaming
- Day 14: Real-Time Data Processing
- Day 20: Kafka Integration
- Day 21: Event-Driven Architecture
- Day 22: Stream Processing Patterns
- Day 26: Monitoring & Alerting
- Day 27: Error Handling & Recovery

**Nice to Have:**
- Day 9: Time Travel & Fail-Safe
- Day 10: Zero-Copy Cloning
- Day 15: Role-Based Access Control (RBAC)
- Day 18: Resource Monitors & Cost Control

### 100 Days of Data and AI
**Mandatory Lessons:**
- Day 21-25: Apache Kafka Fundamentals
- Day 26-30: Stream Processing with Kafka
- Day 31-35: Real-Time Data Pipelines
- Day 41-45: Change Data Capture (CDC)
- Day 71-75: Event-Driven Architecture

**Nice to Have:**
- Day 36-40: Data Quality & Validation
- Day 76-80: Monitoring & Observability

### 30 Days of SQL for Data and AI
**Mandatory Lessons:**
- Day 1-5: SQL Fundamentals
- Day 6-10: Advanced Queries
- Day 23-25: Real-Time Analytics

**Nice to Have:**
- Day 11-14: Data Manipulation
- Day 20-22: Performance Optimization

## Estimated Prerequisites Time
- **Mandatory**: 55-65 hours
- **Nice to Have**: 20-25 hours
- **Total Recommended**: 75-90 hours

## Skills Validation
Before starting this project, ensure you can:
- Configure Snowpipe for automated data loading
- Set up Kafka producers and consumers
- Implement CDC patterns for real-time data sync
- Create and manage Snowflake streams and tasks
- Monitor streaming data pipelines
- Handle errors and implement recovery mechanisms
