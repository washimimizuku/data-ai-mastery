# Prerequisites: Lakehouse Architecture Project

## Required Bootcamps

### 100 Days of Data and AI
**Mandatory Lessons:**
- Day 1-10: Data Processing Fundamentals
- Day 21-25: Columnar Storage (Parquet, ORC)
- Day 26-30: Table Formats (Delta, Iceberg, Hudi)
- Day 31-35: Data Lake Architecture
- Day 36-40: Data Versioning & Time Travel
- Day 41-45: Schema Evolution & Management
- Day 46-50: Data Pipeline Orchestration
- Day 51-55: Apache Spark Fundamentals
- Day 56-60: Spark SQL & DataFrames
- Day 81-85: Data Governance & Security
- Day 86-90: Data Cataloging & Lineage

**Nice to Have:**
- Day 61-65: Advanced Spark Features
- Day 71-75: Event-Driven Architecture
- Day 76-80: Monitoring & Observability

### 60 Days of Advanced Data and AI
**Mandatory Lessons:**
- Day 16-20: Distributed Systems Design
- Day 36-40: Infrastructure as Code
- Day 41-45: Data Warehouse Modernization

**Nice to Have:**
- Day 21-25: Microservices Architecture
- Day 26-30: Container Orchestration
- Day 46-50: Multi-Cloud Strategies

### 30 Days of SQL for Data and AI
**Mandatory Lessons:**
- Day 1-15: SQL Fundamentals & Advanced Queries
- Day 20-22: Performance Optimization

**Nice to Have:**
- Day 16-19: Data Warehousing Concepts
- Day 23-25: Real-Time Analytics

### 30 Days of Python for Data and AI
**Mandatory Lessons:**
- Day 1-15: Python Fundamentals & OOP
- Day 21-25: Data Science Libraries

## Estimated Prerequisites Time
- **Mandatory**: 90-100 hours
- **Nice to Have**: 25-30 hours
- **Total Recommended**: 115-130 hours

## Skills Validation
Before starting this project, ensure you can:
- Design and implement lakehouse architectures
- Work with Delta Lake, Iceberg, and Hudi table formats
- Build data pipelines with Apache Spark
- Implement data governance and cataloging solutions
- Handle schema evolution and data versioning
- Optimize query performance and storage efficiency
