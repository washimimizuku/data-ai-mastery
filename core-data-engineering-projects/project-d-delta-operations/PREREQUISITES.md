# Prerequisites: Delta Lake Operations Project

## Required Bootcamps

### 30 Days of Python for Data and AI
**Mandatory Lessons:**
- Day 1-5: Python Fundamentals
- Day 6-10: Data Structures & Control Flow
- Day 11-15: Functions & Modules
- Day 21-25: Data Science Libraries (NumPy, Pandas)

**Nice to Have:**
- Day 16-20: Object-Oriented Programming

### 100 Days of Data and AI
**Mandatory Lessons:**
- Day 1-5: Data Processing Fundamentals
- Day 21-25: Columnar Storage (Parquet, ORC)
- Day 26-30: Table Formats (Delta, Iceberg, Hudi)
- Day 31-35: Data Lake Architecture
- Day 36-40: Data Versioning & Time Travel
- Day 41-45: Schema Evolution & Management
- Day 46-50: ACID Transactions in Data Lakes

**Nice to Have:**
- Day 16-20: File Formats & Serialization
- Day 51-55: Data Pipeline Orchestration
- Day 81-85: Data Governance Fundamentals

### 30 Days of SQL for Data and AI
**Mandatory Lessons:**
- Day 1-5: SQL Fundamentals
- Day 6-10: Advanced Queries
- Day 11-14: Data Manipulation

**Nice to Have:**
- Day 15-17: Data Security & Access Control
- Day 20-22: Performance Optimization

## Estimated Prerequisites Time
- **Mandatory**: 45-50 hours
- **Nice to Have**: 15-20 hours
- **Total Recommended**: 60-70 hours

## Skills Validation
Before starting this project, ensure you can:
- Understand Delta Lake architecture and ACID properties
- Work with Delta tables using Python (delta-rs, pyspark)
- Implement CRUD operations on Delta tables
- Handle schema evolution and data versioning
- Use time travel and data rollback features
- Optimize Delta table performance (compaction, Z-ordering)
