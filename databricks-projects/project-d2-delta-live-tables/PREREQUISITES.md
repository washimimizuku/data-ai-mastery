# Prerequisites: Delta Live Tables Pipeline Project

## Required Bootcamps

### 30 Days of Databricks for Data and AI
**Mandatory Lessons:**
- Day 1: Databricks Architecture & Workspace Setup
- Day 2: Clusters & Compute Management
- Day 3: Notebooks & Collaborative Development
- Day 10: Delta Lake Fundamentals
- Day 12: Delta Live Tables (DLT) Fundamentals
- Day 13: DLT Pipeline Development
- Day 14: Data Quality & Expectations
- Day 19: Streaming Data Processing
- Day 20: Auto Loader & Incremental Processing
- Day 21: Pipeline Orchestration
- Day 22: Error Handling & Recovery
- Day 26: Pipeline Monitoring & Observability

**Nice to Have:**
- Day 4: Unity Catalog Fundamentals
- Day 7: Access Control & Permissions
- Day 17: Cost Management & Optimization
- Day 18: Performance Monitoring

### 100 Days of Data and AI
**Mandatory Lessons:**
- Day 21-25: Apache Kafka Fundamentals
- Day 31-35: Real-Time Data Pipelines
- Day 36-40: Data Quality & Validation
- Day 41-45: Change Data Capture (CDC)
- Day 46-50: Data Pipeline Orchestration
- Day 71-75: Event-Driven Architecture
- Day 76-80: Monitoring & Observability

**Nice to Have:**
- Day 26-30: Stream Processing with Kafka
- Day 51-55: Data Transformation Patterns
- Day 81-85: Data Governance Fundamentals

### 30 Days of Python for Data and AI
**Mandatory Lessons:**
- Day 1-5: Python Fundamentals
- Day 6-10: Data Structures & Control Flow
- Day 11-15: Functions & Modules
- Day 21-25: Data Science Libraries (NumPy, Pandas)

**Nice to Have:**
- Day 16-20: Object-Oriented Programming
- Day 26-30: Machine Learning with scikit-learn

### 30 Days of SQL for Data and AI
**Mandatory Lessons:**
- Day 1-5: SQL Fundamentals
- Day 6-10: Advanced Queries
- Day 11-14: Data Manipulation
- Day 23-25: Real-Time Analytics

**Nice to Have:**
- Day 15-17: Data Security & Access Control
- Day 20-22: Performance Optimization

## Estimated Prerequisites Time
- **Mandatory**: 70-80 hours
- **Nice to Have**: 20-25 hours
- **Total Recommended**: 90-105 hours

## Skills Validation
Before starting this project, ensure you can:
- Create and manage Delta Lake tables
- Build Delta Live Tables pipelines with Python/SQL
- Implement data quality expectations and constraints
- Configure streaming data ingestion with Auto Loader
- Set up pipeline orchestration and scheduling
- Monitor pipeline performance and handle errors
- Implement incremental data processing patterns
